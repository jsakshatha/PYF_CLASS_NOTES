{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-a7qpNhJF5Mw"
   },
   "source": [
    "# k-Nearest Neighbors\n",
    "\n",
    "K-nearest neighbors (KNN) is a type of supervised learning algorithm which is used for both regression and classification purposes, but mostly it is used for the later. Given a dataset with different classes, KNN tries to predict the correct class of test data by calculating the distance between the test data and all the training points. It then selects the k points which are closest to the test data.  Once the points are selected, the algorithm calculates the probability (in case of classification) of the test point belonging to the classes of the k training points and the class with the highest probability is selected. In the case of a regression problem, the predicted value is the mean of the k selected training points.\n",
    "\n",
    "\n",
    "Let’s understand this with an illustration:\n",
    "\n",
    "\n",
    "1)\tGiven a training dataset as given below. We have a new test data that we need to assign to one of the two classes.\n",
    "\n",
    "<img src=\"1.png\" width=\"\">\n",
    "                                      \n",
    "\n",
    "2)\tNow, the k-NN algorithm calculates the distance between the test data and the given training data.\n",
    "\n",
    "<img src=\"2.png\" width=\"\">\n",
    "\n",
    "                                                        \n",
    "3)\tAfter calculating the distance, it will select the k training points which are nearest to the test data. Let’s assume the value of k is 3 for our example.\n",
    "\n",
    "<img src=\"3.png\" width=\"\">                                            \n",
    "\n",
    "\n",
    "4)\tNow, 3 nearest neighbors are selected, as shown in the figure above. Let’s see in which class our test data will be assigned :\n",
    "\n",
    "Number of Green class values = 2\n",
    "Number of Red class values = 1\n",
    "Probability(Green) = 2/3\n",
    "Probability(Red) = 1/3\n",
    "\n",
    "Since the probability for Green class is higher than Red, the k-NN algorithm will assign the test data to the Green class.\n",
    "\n",
    "Similarly, if this were the case of a regression problem, the predicted value for the test data will simply be the mean of all the 3 nearest values.\n",
    "\n",
    "This is the basic working algorithm for k-NN. Let’s understand how the distance is calculated :\n",
    "\n",
    "### Euclidean Distance: \n",
    "\n",
    "It is the most commonly used method to calculate the distance between two points.\n",
    "The Euclidean distance between two points ‘p(p1,p2)’ and ‘q(q1,q2)’ is calculated  as :\n",
    "\n",
    "<img src=\"4.png\" width=\"\">       image source : Wikipedia\n",
    "\n",
    "<img src=\"5.png\" width=\"\">\n",
    "\n",
    "                                          \n",
    "Similarly,for n-dimensional space, the Euclidean distance is given as :\n",
    "\n",
    "<img src=\"6.png\" width=\"\">\n",
    " \n",
    "\n",
    "### Lazy Learners\n",
    "\n",
    "k-NN algorithms are often termed as Lazy learners. Let’s understand why is that. Most of the algorithms like Bayesian classification, logistic regression, SVM etc., are called Eager learners. These algorithms generalize over the training set before receiving the test data i.e. they create a model based on the training data before receiving the test data and then do the prediction/classification on the test data.\n",
    "But this is not the case with the k-NN algorithm. It doesn’t create a generalized model for the training set but waits for the test data. Once test data is provided then only it starts generalizing the training data to classify the test data.  So, a lazy learner just stores the training data and waits for the test set. Such algorithms work less while training and more while classifying a given test dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Pros and Cons of KNN Algorithm\n",
    "\n",
    "Pros:\n",
    "*\tIt can be used for both regression and classification problems.\n",
    "*\tIt is very simple and easy to implement.\n",
    "*\tMathematics behind the algorithm is easy to understand.\n",
    "*\tThere is no need to create model or do hyperparameter tuning.\n",
    "*   KNN doesn't make any assumption for the distribution of the given data.\n",
    "*   There is not much time cost in training phase.\n",
    "\n",
    "Cons:\n",
    "*\tFinding the optimum value of ‘k’\n",
    "*\tIt takes a lot of time to compute the distance between each test sample and all training samples.\n",
    "*\tSince the model is not saved beforehand in this algorithm (lazy learner), so every time one predicts a test value, it follows the same steps again and again. \n",
    "*\tSince, we need to store the whole training set for every test set, it requires a lot of space.\n",
    "*\tIt is not suitable for high dimensional data.\n",
    "*   Expensive in testing phase\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db5t5rIyrT1R"
   },
   "source": [
    "### Applications of KNN Algorithm\n",
    "\n",
    "- Recommender systems: recommending ads to display to a user (YouTube) or recommending products (Amazon ), or recommending media to consume.  For example, if you buy a smartphone from Amazon, it recommends a mobile cover or earphones to go with it.\n",
    "\n",
    "- KNN used in politics for classifying a potential voter as a “will vote” or “will not vote” candidate.\n",
    "\n",
    "- KNN algorithms can be used to find an individual’s credit rating by comparing with the persons having similar traits.\n",
    "\n",
    "- Other advanced applications of KNN include video recognition, image recognition, and handwriting detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUSI70wiF5M4"
   },
   "source": [
    "### Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Case:-To predict whether a person will have diabetes or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVpKqQLnF5M6"
   },
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor # For regression task\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOOVZAkuF5M8"
   },
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "data = pd.read_csv(\"diabetes.csv\") \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain analysis\n",
    "# EDA and get the insights from data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LfuGMUnfF5M_"
   },
   "outputs": [],
   "source": [
    "# Get the statistical information of the data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQ61ytnBrT1l"
   },
   "outputs": [],
   "source": [
    "# Cheking for null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVXyVqwgF5NA"
   },
   "source": [
    "It seems that there are no missing values in our data. Great, let's see the distribution of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2WR31lRF5NA"
   },
   "outputs": [],
   "source": [
    "# let's see how data is distributed for every column\n",
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in data:\n",
    "    if plotnumber<=9 :     # as there are 9 columns in the data\n",
    "        ax = plt.subplot(3,3,plotnumber)\n",
    "        sns.distplot(data[column])\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "        #plt.ylabel('Salary',fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qLqiKlKF5NB"
   },
   "source": [
    "We can see there is some skewness in the data, let's deal with data.\n",
    "\n",
    "Also, we can see there few data for columns Insulin, skin thickness, BMI and Blood Pressure which have value as 0. You can do a quick search to see that one cannot have 0 values for these.\n",
    "Let's deal with that. we can either remove such data or simply replace it with their respective mean values.\n",
    "Let's do the latter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDtx5J30F5NC"
   },
   "outputs": [],
   "source": [
    "# replacing zero values with the mean of the column\n",
    "data['BMI'] = data['BMI'].replace(0,data['BMI'].median())\n",
    "data['BloodPressure'] = data['BloodPressure'].replace(0,data['BloodPressure'].median())\n",
    "data['Insulin'] = data['Insulin'].replace(0,data['Insulin'].median())\n",
    "data['SkinThickness'] = data['SkinThickness'].replace(0,data['SkinThickness'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heCviQyCF5ND"
   },
   "outputs": [],
   "source": [
    "# let's see how data is distributed for every column\n",
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber = 1\n",
    "\n",
    "for column in data:\n",
    "    if plotnumber<=9 :\n",
    "        ax = plt.subplot(3,3,plotnumber)\n",
    "        sns.distplot(data[column])\n",
    "        plt.xlabel(column,fontsize=20)\n",
    "        #plt.ylabel('Salary',fontsize=20)\n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKQIERUcF5NF"
   },
   "outputs": [],
   "source": [
    "### Split X and Y\n",
    "X = data.drop(columns = ['Outcome']) # Independent variables\n",
    "y = data['Outcome'] # Dependent or target varaible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmG8qZnVF5NG"
   },
   "outputs": [],
   "source": [
    "## scaling the data\n",
    "scalar = StandardScaler()\n",
    "X_scaled = scalar.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "307VbY45F5NH"
   },
   "outputs": [],
   "source": [
    "## splitting the training and testing data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled,y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bcDZ0_XF5NI"
   },
   "outputs": [],
   "source": [
    "## taking optimal k to determine how many nearest neighbors  to create\n",
    "\n",
    "# create a list to store the error values for each k\n",
    "error_rate = []\n",
    "\n",
    "# Will take some time\n",
    "for i in range(1,11):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U43NmzLUF5NJ"
   },
   "outputs": [],
   "source": [
    "# Lets plot the k-value and error rate\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,11),error_rate,color='blue', linestyle='dashed', \n",
    "         marker='o',markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHezz_G2F-J4"
   },
   "outputs": [],
   "source": [
    "## This is reference code donot run this on current dataset.\n",
    "## Error calculation for regression task\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse_val = [] #create list to store rmse values for different k\n",
    "for K in range(1,20):\n",
    "    \n",
    "    model = KNeighborsRegressor(n_neighbors = K)\n",
    "\n",
    "    model.fit(X_train, y_train)  #fit the model\n",
    "    pred=model.predict(X_test) #make prediction on test set\n",
    "    error = np.sqrt(mean_squared_error(y_test,pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    print('RMSE value for k= ' , K , 'is:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NejgmfuSF5NK"
   },
   "outputs": [],
   "source": [
    "# let's fit the data into KNN model and see how well it performs:\n",
    "knn1 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTPOqgX9F5NL"
   },
   "outputs": [],
   "source": [
    "# Predict \n",
    "y_pred = knn1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDMOHOCEF5NM"
   },
   "outputs": [],
   "source": [
    "# Checking Accuracy score\n",
    "print(\"The accuracy score is : \", accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADRyUgy9F5NN"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall1=recall_score(y_test,y_pred)\n",
    "recall1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZRU3ZmbAF5NO"
   },
   "outputs": [],
   "source": [
    "## checking the balance of traget\n",
    "# Imbalanced dataset\n",
    "data.Outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCjZ0zL2F5NP"
   },
   "source": [
    "### Balancing the data-SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1ewmWO_F5NP"
   },
   "source": [
    "Training a machine learning model on an imbalanced dataset can introduce unique challenges to the learning problem. Imbalanced data typically refers to a classification problem where the number of observations per class is not equally distributed; often you'll have a large amount of data/observations for one class (referred to as the majority class), and much fewer observations for one or more other classes (referred to as the minority classes). For example, suppose you're building a classifier to classify a credit card transaction a fraudulent or authentic - you'll likely have 10,000 authentic transactions for every 1 fraudulent transaction, that's quite an imbalance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKGz3FA_F5NQ"
   },
   "source": [
    "### The imbalance to the class distribution in an imbalanced classification predictive modeling problem may have many causes.\n",
    "\n",
    "There are perhaps two main groups of causes for the imbalance we may want to consider; they are data sampling and properties of the domain.\n",
    "\n",
    "It is possible that the imbalance in the examples across the classes was caused by the way the examples were collected or sampled from the problem domain. This might involve biases introduced during data collection, and errors made during data collection.\n",
    "\n",
    "Biased Sampling.\n",
    "Measurement Errors.\n",
    "For example, perhaps examples were collected from a narrow geographical region, or slice of time, and the distribution of classes may be quite different or perhaps even collected in a different way.\n",
    "\n",
    "Errors may have been made when collecting the observations. One type of error might have been applying the wrong class labels to many examples. Alternately, the processes or systems from which examples were collected may have been damaged or impaired to cause the imbala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upxrdvGwF5NQ"
   },
   "source": [
    "Imbalance can be handled in dataset by apply certain techniques like\n",
    "\n",
    "1)OverSampling 2)Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrsPULrKF5NS"
   },
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUMxtruXF5NS"
   },
   "source": [
    "Synthetic Minority Over-sampling Technique (SMOTE) is a technique that generates new observations by interpolating between observations in the original dataset.\n",
    "Interpolation is done with the help of KNN alogrith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFZfZW32F5NT"
   },
   "outputs": [],
   "source": [
    "!pip install imblearn\n",
    "\n",
    "# After installing library restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7jZZUB7F5NT"
   },
   "outputs": [],
   "source": [
    "#!pip install delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dntzii3mF5NU"
   },
   "outputs": [],
   "source": [
    "##conda install -c glemaitre imbalanced-learn -- on command prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrU6_DvhF5NV"
   },
   "outputs": [],
   "source": [
    "conda install --user glemaitre imbalanced-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oei7T0THF5NX"
   },
   "outputs": [],
   "source": [
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCjoEzcJF5NZ"
   },
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE() ## object creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hE_nMaKQF5Nb"
   },
   "outputs": [],
   "source": [
    "## Do not SMOTE the testing data.Because when the model is on production,\n",
    "#the data may or may not be balanced one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYBG3Tn7F5Ne"
   },
   "outputs": [],
   "source": [
    "X_train_smote, y_train_smote = smote.fit_resample(X_train.astype('float'),\n",
    "                                                  y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61BMn9KbF5Ng"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(\"Actual Classes\",Counter(y_train))\n",
    "print(\"SMOTE Classes\",Counter(y_train_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9FlS2DCF5Nh"
   },
   "outputs": [],
   "source": [
    "\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn2.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXykNPo9F5Nj"
   },
   "outputs": [],
   "source": [
    "# Predict the output for X_test\n",
    "y_pred = knn2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "its8bJD3F5Nk"
   },
   "outputs": [],
   "source": [
    "print(\"The accuracy score is : \", accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPyxOy_pF5Nl"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybOSZODrrT2q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5ouCcnkrT2r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
